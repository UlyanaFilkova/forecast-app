<template>
  <div class="linear-regression-info">
    <h1>Метод линейной регрессии</h1>
    <section>
      <p>
        Метод <b>линейной регрессии</b> — это один из самых простых и широко используемых методов
        машинного обучения, основанный на статистике. В его основе лежит идея построения
        <b>прямой линии</b>, которая наилучшим образом описывает связь между входными данными
        (факторами) и целевой переменной.
      </p>
      <h2>Основная идея линейной регрессии</h2>
      <p>
        Линейная регрессия предполагает, что зависимость между входными переменными (\(x\)) и
        целевой переменной (\(y\)) можно выразить линейным уравнением:
      </p>
      <p class="formula">\( y = w_0 + w_1 x_1 + w_2 x_2 + \dots + w_n x_n + \epsilon \)</p>
      <p>Где:</p>
      <ul>
        <li>\(y\) — целевая переменная (предсказываемое значение).</li>
        <li>\(x_1, x_2, \dots, x_n\) — объясняющие переменные (факторы или признаки).</li>
        <li>\(w_0\) — свободный член (пересечение линии с осью \(y\)).</li>
        <li>
          \(w_1, w_2, \dots, w_n\) — коэффициенты (веса), которые показывают, как каждый признак
          влияет на \(y\).
        </li>
        <li>\(\epsilon\) — ошибка или шум (разница между реальным и предсказанным значениями).</li>
      </ul>
      <p>
        Цель линейной регрессии — подобрать такие коэффициенты (\(w_0, w_1, \dots, w_n\)), чтобы
        модель наилучшим образом объясняла данные.
      </p>
      <h2>Простая линейная регрессия</h2>
      <p>Это частный случай, когда есть только один входной признак (\(x\)):</p>
      <p class="formula">\( y = w_0 + w_1 x \)</p>
      <p>
        На графике это прямая линия. Задача состоит в том, чтобы найти её оптимальный наклон
        (\(w_1\)) и точку пересечения (\(w_0\)).
      </p>
      <h2>Множественная линейная регрессия</h2>
      <p>
        Если у нас есть несколько признаков (\(x_1, x_2, \dots, x_n\)), то модель становится
        многомерной:
      </p>
      <p class="formula">\( y = w_0 + w_1 x_1 + w_2 x_2 + \dots + w_n x_n \)</p>
      <p>
        В этом случае задача состоит в нахождении гиперплоскости, которая минимизирует отклонение
        между предсказанными и реальными значениями.
      </p>
      <h2>Как подбираются коэффициенты?</h2>
      <p>
        Коэффициенты (\(w_0, w_1, \dots, w_n\)) подбираются так, чтобы минимизировать сумму
        квадратов ошибок (\(RSS\)):
      </p>
      <p class="formula">\( RSS = \sum_{i=1}^m (y_i - (\hat{y}_i))^2 \)</p>
      <p>Где:</p>
      <ul>
        <li>\(m\) — количество наблюдений.</li>
        <li>\(y_i\) — реальное значение.</li>
        <li>
          \(\hat{y}_i = w_0 + w_1 x_{i1} + w_2 x_{i2} + \dots + w_n x_{in}\) — предсказанное
          значение.
        </li>
      </ul>
      <p>Для нахождения минимального значения RSS используются математические методы, такие как:</p>
      <ul>
        <li>
          <b>Метод градиентного спуска</b> — итеративный процесс поиска оптимальных коэффициентов.
        </li>
        <li><b>Метод нормальных уравнений</b> — аналитическое решение системы уравнений.</li>
      </ul>
      <h2>Основные допущения линейной регрессии</h2>
      <ul>
        <li>
          <b>Линейность зависимости:</b> предполагается, что связь между признаками (\(x\)) и
          целевой переменной (\(y\)) линейная.
        </li>
        <li><b>Независимость ошибок:</b> ошибки (\(\epsilon\)) не должны быть взаимосвязаны.</li>
        <li>
          <b>Гомоскедастичность ошибок:</b> дисперсия ошибок постоянна для всех значений \(x\).
        </li>
        <li><b>Нормальность ошибок:</b> ошибки имеют нормальное распределение.</li>
        <li>
          <b>Отсутствие мультиколлинеарности:</b> признаки не должны быть сильно коррелированы между
          собой.
        </li>
      </ul>
      <h2>Преимущества линейной регрессии</h2>
      <ul>
        <li><b>Простота:</b> легко реализуется и интерпретируется.</li>
        <li><b>Высокая скорость:</b> подходит для работы с большими объёмами данных.</li>
        <li>
          <b>Интерпретируемость:</b> коэффициенты показывают влияние каждого признака на результат.
        </li>
        <li>
          <b>Универсальность:</b> используется в прогнозировании, анализе трендов, маркетинге и
          экономике.
        </li>
      </ul>
      <h2>Ограничения линейной регрессии</h2>
      <ul>
        <li>
          <b>Линейное предположение:</b> модель плохо работает, если зависимость между переменными
          нелинейная.
        </li>
        <li><b>Чувствительность к выбросам:</b> выбросы могут сильно исказить результаты.</li>
        <li>
          <b>Зависимость от корректности данных:</b> модель чувствительна к масштабированию
          признаков и корреляциям между ними.
        </li>
        <li>
          <b>Неприменимость к сложным данным:</b> плохо работает с большими наборами данных с
          нелинейной структурой.
        </li>
      </ul>
    </section>
  </div>
</template>

<script>
export default {
  name: 'LinearRegressionInfo',
  mounted() {
    // Перерисовка формул MathJax после монтирования
    if (window.MathJax) {
      window.MathJax.typesetPromise()
    }
  },
}
</script>

<style scoped>
.linear-regression-info {
  font-family: Arial, sans-serif;
  line-height: 1.6;
  margin: 20px;
  padding: 20px;
  background-color: #fffdfd;
  border-radius: 8px;
  box-shadow: 0 2px 6px rgba(0, 0, 0, 0.3);
}

h1 {
  font-size: 24px;
  color: #333;
}

h2 {
  font-size: 20px;
  text-align: center;
  color: #0057b4;
}

p,
ul {
  color: #555;
}
li {
  margin-bottom: 5px;
}

.formula {
  font-size: 1.2em;
  text-align: center;
  margin: 20px 0;
  color: #000;
}
</style>
