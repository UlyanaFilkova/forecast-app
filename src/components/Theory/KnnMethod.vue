<template>
  <div class="method-info">
    <h1>Метод K ближайших соседей</h1>
    <section>
      <p>
        <b>Метод K ближайших соседей (K-Nearest Neighbors, KNN)</b> — это простой и эффективный
        алгоритм машинного обучения, основанный на поиске ближайших соседей по некоторой метрике
        расстояния.
      </p>
      <p>
        KNN не требует явного обучения: он сохраняет все данные и принимает решения на этапе
        предсказания, что делает его примером <b>ленивого обучения</b> (lazy learning).
      </p>
    </section>

    <section>
      <h2>Как работает метод k-ближайших соседей?</h2>
      <p>Для нового объекта KNN выполняет следующие шаги:</p>
      <ol>
        <li>Вычисляет расстояния до всех точек обучающей выборки.</li>
        <li>Сортирует обучающие точки по возрастанию расстояния до нового объекта.</li>
        <li>Выбирает <b>K</b> ближайших соседей (гиперпараметр метода).</li>
        <li>
          Решает задачу:
          <ul>
            <li>
              Для <b>классификации</b>: выбирается класс, встречающийся чаще всего среди K соседей
              (метод голосования).
            </li>
            <li>
              Для <b>регрессии</b>: вычисляется среднее значение целевых переменных K соседей.
            </li>
          </ul>
        </li>
      </ol>
    </section>

    <section>
      <h2>Основные параметры метода</h2>
      <ul>
        <li>
          <b>K — количество соседей:</b> Чем меньше K, тем более детально метод будет учитывать
          локальные особенности данных, но тем сильнее он подвержен шуму. Увеличение K делает метод
          более устойчивым, но может потерять локальные детали.
        </li>
        <li>
          <b>Метрика расстояния:</b> Используются различные метрики:
          <ul>
            <li>
              Евклидово расстояние: \( d(p, q) = \sqrt{\sum_{i=1}^n (p_i - q_i)^2} \) – для данных с
              непрерывными значениями.
            </li>
            <li>
              Манхэттенское расстояние: \( d(p, q) = \sum_{i=1}^n |p_i - q_i| \) – если данные
              представляют собой прямоугольную сетку.
            </li>
            <li>
              Косинусное расстояние: \( d(p, q) = 1 - \cos(\theta) \), где \(\theta\) – угол между
              векторами. Для текстовых данных или векторов.
            </li>
          </ul>
        </li>
      </ul>
    </section>

    <section>
      <h2>Визуализация работы KNN</h2>
      <p>
        На 2D-графике точки обучающей выборки разделены по классам. Для новой точки KNN определяет
        ближайших соседей и относит точку к одному из классов:
      </p>
      <img src="@/assets/KnnClassification.svg" alt="" srcset="" />
      <ul>
        <li>Красные и синие точки — это классы обучающей выборки.</li>
        <li>Зеленый кружок — новый объект.</li>
      </ul>
    </section>

    <section>
      <h2>Пример работы KNN (на интуитивном уровне)</h2>
      <h3>Классификация:</h3>
      <p>Есть обучающая выборка:</p>
      <ul>
        <li>(2, 3, "Класс A"),</li>
        <li>(5, 4, "Класс B"),</li>
        <li>(1, 2, "Класс A"),</li>
        <li>(6, 5, "Класс B"),</li>
      </ul>
      <p>Получаем новый объект: (3, 3)</p>
      <p>Находим расстояния до всех точек:</p>
      <ul>
        <li>До (2, 3): \( (3 - 2)^2 + (3 - 3)^2 = 1 \)</li>
        <li>До (5, 4): \( (5 - 3)^2 + (4 - 3)^2 = 2.24 \)</li>
        <li>До (1, 2): \( (3 - 1)^2 + (3 - 2)^2 = 2.24 \)</li>
        <li>До (6, 5): \( (6 - 3)^2 + (5 - 3)^2 = 3.61 \)</li>
      </ul>
      <p>Сортируем: (2, 3), (5, 4), (1, 2), (6, 5).</p>
      <p>
        Берем <b>K = 3</b>: ближайшие соседи — (2, 3, "Класс A"), (5, 4, "Класс B"), (1, 2, "Класс
        A").
      </p>
      <p><b>Голосуем:</b></p>
      <ul>
        <li>Класс A: 2</li>
        <li>Класс B: 1</li>
      </ul>
      <p><b>Ответ:</b> "Класс A".</p>
    </section>

    <section>
      <h2>Преимущества метода KNN</h2>
      <ul>
        <li>Простота реализации и интерпретации.</li>
        <li>Подходит как для классификации, так и для регрессии.</li>
        <li>Не требует этапа обучения и сложной настройки.</li>
        <li>Гибкость: можно использовать разные метрики расстояния.</li>
      </ul>
    </section>

    <section>
      <h2>Недостатки метода KNN</h2>
      <ul>
        <li>Медленный при большом количестве данных (требует расчёта расстояний до всех точек).</li>
        <li>Чувствителен к выбору гиперпараметра K.</li>
        <li>Требует нормализации данных из-за влияния масштаба признаков.</li>
        <li>Неэффективен в задачах с высокой размерностью (проклятие размерности).</li>
      </ul>
    </section>

    <section>
      <h2>Применение метода KNN</h2>
      <ul>
        <li>Распознавание изображений (например, рукописных цифр).</li>
        <li>Диагностика в медицине (например, классификация заболеваний).</li>
        <li>Рекомендательные системы (поиск схожих пользователей или товаров).</li>
        <li>Прогнозирование поведения пользователей.</li>
      </ul>
    </section>
  </div>
</template>

<script>
export default {
  name: 'KNNInfo',
  mounted() {
    if (window.MathJax) {
      window.MathJax.typesetPromise()
    }
  },
}
</script>

<style scoped>
.method-info {
  font-family: Arial, sans-serif;
  line-height: 1.6;
  margin: 20px;
  padding: 20px;
  background-color: #f9f9f9;
  border-radius: 8px;
  box-shadow: 0 2px 6px rgba(0, 0, 0, 0.1);
}

h1 {
  font-size: 24px;
  color: #333;
  margin-bottom: 10px;
}

h2 {
  font-size: 20px;
  text-align: center;
  color: #0057b4;
  margin-top: 20px;
}

p,
ul,
ol {
  color: #555;
}

li {
  margin-bottom: 8px;
}

img {
  display: block;
  width: 40%;
  margin: 0 auto;
}
</style>
